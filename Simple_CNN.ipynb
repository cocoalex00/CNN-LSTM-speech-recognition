{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a5b784-4299-423f-80ef-a7632ff8f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import helper_functions\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e1edfc-46e8-4bbe-a5ad-fbd75b232d85",
   "metadata": {},
   "source": [
    "## Extracting and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f6f068-43ae-4597-9396-4acf05b61be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 11 11 ... 11 11 11]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "train_data = helper_functions.load_data()\n",
    "\n",
    "X = np.array(train_data['mfccs'], dtype=float)\n",
    "y = np.array([helper_functions.one_hot_to_label(label) for label in train_data['classes']])\n",
    "y_train_one_hot = np.array(train_data['classes'])\n",
    "y = y_train_one_hot.argmax(axis=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28e994-9083-4632-9750-1e08cb96e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "# convert inputs which are 2 dimensional MFCC arrays into 3 dimensional arrays\n",
    "# Currently shape of the arrays is (# segments, 13), we want to add a 3rd dimension so we have (# segments, 13, 1)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_validation = X_validation[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "n_dim = X_train.shape[2]\n",
    "\n",
    "print(f'x_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'n_dim: {n_dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040e022-7581-416e-a796-38090d30c51a",
   "metadata": {},
   "source": [
    "## Building and Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7051dad-3aa6-4d74-a1ad-8dc3dff5c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 13, 171, 64)       640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 13, 171, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 7, 86, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 84, 32)         18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5, 84, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 41, 32)         4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 41, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 21, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                43072     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 35)                2275      \n",
      "=================================================================\n",
      "Total params: 69,091\n",
      "Trainable params: 68,835\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "\n",
    "# build network architecture using convolutional layers\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# 1st conv layer\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "# 2nd conv layer\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "# 3rd conv layer\n",
    "model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "# flatten output and feed into dense layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "# softmax output layer\n",
    "model.add(tf.keras.layers.Dense(35, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3285b-9c1e-4e92-ac10-d5d779d2674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import FalsePositives, FalseNegatives, TruePositives, TrueNegatives, Precision, Recall\n",
    "\n",
    "optimiser = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimiser, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy', FalsePositives(), FalseNegatives(), TruePositives(), TrueNegatives(), Precision(), Recall()])\n",
    "\n",
    "# print model parameters on console\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72ab492-dc59-4ee1-8fe9-10cbb81a516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1639/1639 [==============================] - 486s 296ms/step - loss: 1.8855 - accuracy: 0.5733 - val_loss: 1.0270 - val_accuracy: 0.7008\n",
      "Epoch 2/40\n",
      "1639/1639 [==============================] - 206s 126ms/step - loss: 0.9337 - accuracy: 0.7296 - val_loss: 0.7441 - val_accuracy: 0.7758\n",
      "Epoch 3/40\n",
      "1639/1639 [==============================] - 175s 107ms/step - loss: 0.6893 - accuracy: 0.8038 - val_loss: 0.6241 - val_accuracy: 0.8156\n",
      "Epoch 4/40\n",
      "1639/1639 [==============================] - 197s 120ms/step - loss: 0.5341 - accuracy: 0.8538 - val_loss: 0.5168 - val_accuracy: 0.8555\n",
      "Epoch 5/40\n",
      "1639/1639 [==============================] - 252s 154ms/step - loss: 0.4538 - accuracy: 0.8802 - val_loss: 0.4743 - val_accuracy: 0.8699\n",
      "Epoch 6/40\n",
      "1639/1639 [==============================] - 149s 90ms/step - loss: 0.3943 - accuracy: 0.8984 - val_loss: 0.4521 - val_accuracy: 0.8757\n",
      "Epoch 7/40\n",
      "1639/1639 [==============================] - 149s 91ms/step - loss: 0.3570 - accuracy: 0.9074 - val_loss: 0.4270 - val_accuracy: 0.8879\n",
      "Epoch 8/40\n",
      "1639/1639 [==============================] - 164s 100ms/step - loss: 0.3217 - accuracy: 0.9174 - val_loss: 0.4540 - val_accuracy: 0.8735\n",
      "Epoch 9/40\n",
      "1639/1639 [==============================] - 149s 90ms/step - loss: 0.2985 - accuracy: 0.9265 - val_loss: 0.3819 - val_accuracy: 0.8992\n",
      "Epoch 10/40\n",
      "1639/1639 [==============================] - 174s 106ms/step - loss: 0.2729 - accuracy: 0.9340 - val_loss: 0.3792 - val_accuracy: 0.9008\n",
      "Epoch 11/40\n",
      "1639/1639 [==============================] - 140s 85ms/step - loss: 0.2552 - accuracy: 0.9398 - val_loss: 0.3479 - val_accuracy: 0.9106\n",
      "Epoch 12/40\n",
      "1639/1639 [==============================] - 168s 103ms/step - loss: 0.2457 - accuracy: 0.9415 - val_loss: 0.3246 - val_accuracy: 0.9147\n",
      "Epoch 13/40\n",
      "1639/1639 [==============================] - 151s 92ms/step - loss: 0.2320 - accuracy: 0.9456 - val_loss: 0.3281 - val_accuracy: 0.9145\n",
      "Epoch 14/40\n",
      "1639/1639 [==============================] - 139s 84ms/step - loss: 0.2174 - accuracy: 0.9495 - val_loss: 0.3618 - val_accuracy: 0.9051\n",
      "Epoch 15/40\n",
      "1639/1639 [==============================] - 155s 94ms/step - loss: 0.2075 - accuracy: 0.9528 - val_loss: 0.3083 - val_accuracy: 0.9205\n",
      "Epoch 16/40\n",
      "1639/1639 [==============================] - 154s 94ms/step - loss: 0.1927 - accuracy: 0.9568 - val_loss: 0.3273 - val_accuracy: 0.9142\n",
      "Epoch 17/40\n",
      "1639/1639 [==============================] - 137s 83ms/step - loss: 0.1851 - accuracy: 0.9580 - val_loss: 0.3099 - val_accuracy: 0.9195\n",
      "Epoch 18/40\n",
      "1639/1639 [==============================] - 130s 79ms/step - loss: 0.1789 - accuracy: 0.9604 - val_loss: 0.2950 - val_accuracy: 0.9250\n",
      "Epoch 19/40\n",
      "1639/1639 [==============================] - 107s 65ms/step - loss: 0.1685 - accuracy: 0.9636 - val_loss: 0.3035 - val_accuracy: 0.9245\n",
      "Epoch 20/40\n",
      "1639/1639 [==============================] - 125s 76ms/step - loss: 0.1598 - accuracy: 0.9659 - val_loss: 0.2911 - val_accuracy: 0.9264\n",
      "Epoch 21/40\n",
      "1639/1639 [==============================] - 132s 80ms/step - loss: 0.1552 - accuracy: 0.9672 - val_loss: 0.2919 - val_accuracy: 0.9236\n",
      "Epoch 22/40\n",
      "1639/1639 [==============================] - 102s 62ms/step - loss: 0.1443 - accuracy: 0.9708 - val_loss: 0.2942 - val_accuracy: 0.9296\n",
      "Epoch 23/40\n",
      "1639/1639 [==============================] - 110s 67ms/step - loss: 0.1378 - accuracy: 0.9730 - val_loss: 0.2888 - val_accuracy: 0.9239\n",
      "Epoch 24/40\n",
      "1639/1639 [==============================] - 141s 86ms/step - loss: 0.1359 - accuracy: 0.9724 - val_loss: 0.3006 - val_accuracy: 0.9272\n",
      "Epoch 25/40\n",
      "1639/1639 [==============================] - 139s 85ms/step - loss: 0.1287 - accuracy: 0.9750 - val_loss: 0.2939 - val_accuracy: 0.9291\n",
      "Epoch 26/40\n",
      "1639/1639 [==============================] - 114s 70ms/step - loss: 0.1210 - accuracy: 0.9784 - val_loss: 0.3029 - val_accuracy: 0.9265\n",
      "Epoch 27/40\n",
      "1639/1639 [==============================] - 150s 91ms/step - loss: 0.1191 - accuracy: 0.9776 - val_loss: 0.3057 - val_accuracy: 0.9289\n",
      "Epoch 28/40\n",
      "1639/1639 [==============================] - 146s 89ms/step - loss: 0.1132 - accuracy: 0.9797 - val_loss: 0.3038 - val_accuracy: 0.9270\n",
      "Epoch 29/40\n",
      "1639/1639 [==============================] - 122s 74ms/step - loss: 0.1109 - accuracy: 0.9795 - val_loss: 0.3256 - val_accuracy: 0.9221\n",
      "Epoch 30/40\n",
      "1639/1639 [==============================] - 137s 83ms/step - loss: 0.1102 - accuracy: 0.9793 - val_loss: 0.3168 - val_accuracy: 0.9246\n",
      "Epoch 31/40\n",
      "1639/1639 [==============================] - 115s 70ms/step - loss: 0.1033 - accuracy: 0.9818 - val_loss: 0.3007 - val_accuracy: 0.9288\n",
      "Epoch 32/40\n",
      "1639/1639 [==============================] - 122s 74ms/step - loss: 0.0976 - accuracy: 0.9837 - val_loss: 0.3049 - val_accuracy: 0.9289\n",
      "Epoch 33/40\n",
      "1639/1639 [==============================] - 127s 77ms/step - loss: 0.0982 - accuracy: 0.9826 - val_loss: 0.3168 - val_accuracy: 0.9224\n",
      "Epoch 34/40\n",
      "1639/1639 [==============================] - 128s 78ms/step - loss: 0.0917 - accuracy: 0.9853 - val_loss: 0.2975 - val_accuracy: 0.9303\n",
      "Epoch 35/40\n",
      "1639/1639 [==============================] - 129s 79ms/step - loss: 0.0884 - accuracy: 0.9858 - val_loss: 0.3194 - val_accuracy: 0.9246\n",
      "Epoch 36/40\n",
      "1639/1639 [==============================] - 127s 77ms/step - loss: 0.0842 - accuracy: 0.9874 - val_loss: 0.3062 - val_accuracy: 0.9282\n",
      "Epoch 37/40\n",
      "1639/1639 [==============================] - 123s 75ms/step - loss: 0.0838 - accuracy: 0.9861 - val_loss: 0.3264 - val_accuracy: 0.9258\n",
      "Epoch 38/40\n",
      "1639/1639 [==============================] - 122s 75ms/step - loss: 0.0808 - accuracy: 0.9877 - val_loss: 0.3150 - val_accuracy: 0.9296\n",
      "Epoch 39/40\n",
      "1639/1639 [==============================] - 126s 77ms/step - loss: 0.0768 - accuracy: 0.9891 - val_loss: 0.3579 - val_accuracy: 0.9212\n",
      "Epoch 40/40\n",
      "1639/1639 [==============================] - 125s 76ms/step - loss: 0.0755 - accuracy: 0.9891 - val_loss: 0.3355 - val_accuracy: 0.9210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efd7c791640>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_validation, y_validation))\n",
    "history = history.history # discard training params - unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38462c-184c-4ccb-8616-dac9f3f0c4c1",
   "metadata": {},
   "source": [
    "## Saving and Loading The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301e1701-0c03-4572-8cb9-e52f2e0bd705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/ross_cnn/assets\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import pickle\n",
    "\n",
    "model_dir = 'models/ross_cnn'\n",
    "history_path = 'models/ross_cnn_history'\n",
    "\n",
    "def save_model(override=False):\n",
    "    if override or not path.exists(model_dir):\n",
    "        model.save(model_dir)\n",
    "def save_history(override=False):\n",
    "    if override or not path.exists(history_path):\n",
    "        file = open(history_path, 'wb')\n",
    "        pickle.dump(history, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58896e9-eba9-4e0b-8c67-bb17119b89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(override=True)\n",
    "save_history(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a2705-96a7-46d4-981c-a8eb860adb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model exists\n",
    "# If not, loaded from saved\n",
    "if (not 'model' in locals() or model == None):\n",
    "    if path.exists(model_dir):\n",
    "        model = tf.keras.models.load_model(model_dir)\n",
    "    else:\n",
    "        raise ValueError('Model doesn\\'t exist and cannot be recovered from disk.')\n",
    "else:\n",
    "    print('Model already loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0427fb7-c1e7-4b75-9382-5577a026afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load history for plotting\n",
    "if (not 'history' in locals() or history == None):\n",
    "    if path.exists(history_path):\n",
    "        file = open(history_path, \"rb\")\n",
    "        history = pickle.load(file)\n",
    "        file.close()\n",
    "    else:\n",
    "        raise ValueError('History doesn\\'t exist and cannot be recovered from disk.')\n",
    "else:\n",
    "    print('History already loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46958626-71ef-4af4-9c25-15fed0b4c22e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803adee1-c292-413a-8e11-e45804a5e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3f992-f30c-49dd-9de7-fa880ade9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show history\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8f7eb-21b5-4aa7-819a-9931ffb65e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aaf028-b6ac-4b19-abd0-7193f993fae0",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2688c-30b2-4ec2-afe5-4823ea07cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let the model predict\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b822a-ef1d-4020-ab8d-91f8cfa748b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then calculate the confusion matrix based on predictions\n",
    "y_test_cm = y_test.argmax(axis=1)\n",
    "y_pred_cm = y_pred.argmax(axis=1)\n",
    "cm = confusion_matrix(y_test_cm, y_pred_cm)\n",
    "\n",
    "fig_dim = 15\n",
    "fig_size = (fig_dim, fig_dim)\n",
    "fig, ax = plt.subplots(figsize=fig_size)\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(helper_functions.PERMITTED_LABELS); ax.yaxis.set_ticklabels(helper_functions.PERMITTED_LABELS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47d642-c1a5-4e3f-ad1a-9aebb0751f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(12):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, y_pred,average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.5f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
